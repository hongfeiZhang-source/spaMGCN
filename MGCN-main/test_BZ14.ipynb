{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.misc import *\n",
    "from train_single.train import Train,Test\n",
    "from datasets.data_utils import load_dataset\n",
    "from model_single.Creat_model import creat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.backends import cudnn\n",
    "def fix_seed(seed):\n",
    "    #seed = 2023\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "    \n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "import torch\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from munkres import Munkres\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import adjusted_rand_score as ari_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score as nmi_score\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import Dataset\n",
    "import argparse\n",
    "import yaml\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "import pynvml\n",
    "\n",
    "def build_args():\n",
    "    \n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('--dataset', type=str, default=\"acm\")\n",
    "    parser.add_argument('--seed', type=int, default=20)\n",
    "    parser.add_argument('--cuda', type=bool, default=True)\n",
    "    parser.add_argument('--n_input', type=int, default=None)\n",
    "    parser.add_argument('--n_z', type=int, default=None)\n",
    "    parser.add_argument('--freedom_degree', type=float, default=1.0)\n",
    "    parser.add_argument('--epoch', type=int, default=None)\n",
    "    parser.add_argument('--shuffle', type=bool, default=True)\n",
    "    parser.add_argument('--sigma', type=float, default=None)\n",
    "    parser.add_argument('--loss_n', type=float, default=None)\n",
    "    parser.add_argument('--loss_w', type=float, default=None)\n",
    "    parser.add_argument('--loss_s', type=float, default=None)\n",
    "    parser.add_argument('--loss_a', type=float, default=None)\n",
    "    parser.add_argument('--lr', type=float, default=1e-4)\n",
    "    parser.add_argument('--acc', type=float, default=-1)\n",
    "    parser.add_argument('--f1', type=float, default=-1)\n",
    "    args = parser.parse_args([])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(adata, use_reps=None, n_comps=10):\n",
    "    \n",
    "    \"\"\"Dimension reduction with PCA algorithm\"\"\"\n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "    from scipy.sparse.csc import csc_matrix\n",
    "    from scipy.sparse.csr import csr_matrix\n",
    "    pca = PCA(n_components=n_comps)\n",
    "    if use_reps is not None:\n",
    "       feat_pca = pca.fit_transform(adata.obsm[use_reps])\n",
    "    else: \n",
    "       if isinstance(adata.X, csc_matrix) or isinstance(adata.X, csr_matrix):\n",
    "          feat_pca = pca.fit_transform(adata.X.toarray()) \n",
    "       else:   \n",
    "          feat_pca = pca.fit_transform(adata.X)\n",
    "    \n",
    "    return feat_pca\n",
    "\n",
    "def clr_normalize_each_cell(adata, inplace=True):\n",
    "    \n",
    "    \"\"\"Normalize count vector for each cell, i.e. for each row of .X\"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    import scipy\n",
    "\n",
    "    def seurat_clr(x):\n",
    "        # TODO: support sparseness\n",
    "        s = np.sum(np.log1p(x[x > 0]))\n",
    "        exp = np.exp(s / len(x))\n",
    "        return np.log1p(x / exp)\n",
    "\n",
    "    if not inplace:\n",
    "        adata = adata.copy()\n",
    "    \n",
    "    # apply to dense or sparse matrix, along axis. returns dense matrix\n",
    "    adata.X = np.apply_along_axis(\n",
    "        seurat_clr, 1, (adata.X.A if scipy.sparse.issparse(adata.X) else np.array(adata.X))\n",
    "    )\n",
    "    return adata     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load configs\n"
     ]
    }
   ],
   "source": [
    "args = build_args()\n",
    "args = load_configs(args, \"config/configs.yml\")\n",
    "# set_random_seed(2024)\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "adata_omics1=sc.read_h5ad(\"/home/hfzhang/data/空间转录组/data1/BZ14/raw/adata.h5ad\")\n",
    "adata_omics1.obsm['spatial']=adata_omics1.obsm['spatial'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3600/529664883.py:6: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  from scipy.sparse.csc import csc_matrix\n",
      "/tmp/ipykernel_3600/529664883.py:7: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  from scipy.sparse.csr import csr_matrix\n"
     ]
    }
   ],
   "source": [
    "adata_omics1 = clr_normalize_each_cell(adata_omics1)\n",
    "sc.pp.scale(adata_omics1)\n",
    "adata_omics1.obsm['feat'] = pca(adata_omics1, n_comps=adata_omics1.n_vars-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 16:18:33.815126: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-20 16:18:34.024485: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-20 16:18:34.640036: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:\n",
      "2025-03-20 16:18:34.640130: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:\n",
      "2025-03-20 16:18:34.640140: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocess import *\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def transform_adjacent_matrix(adjacent):\n",
    "    n_spot = max(adjacent['x'].max(), adjacent['y'].max()) + 1\n",
    "    \n",
    "    # Ensure no indices exceed the matrix dimensions\n",
    "    mask = (adjacent['x'] < n_spot) & (adjacent['y'] < n_spot)\n",
    "    adjacent['x'] = adjacent['x'][mask]\n",
    "    adjacent['y'] = adjacent['y'][mask]\n",
    "    adjacent['value'] = adjacent['value'][mask]\n",
    "    \n",
    "    adj = coo_matrix((adjacent['value'], (adjacent['x'], adjacent['y'])), shape=(n_spot, n_spot))\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adj(adata):\n",
    "    cell_position_omics1 = adata.obsm['spatial']\n",
    "    adj_omics1 = construct_graph_by_coordinate(cell_position_omics1, n_neighbors=3)\n",
    "    adata.uns['adj_spatial'] = adj_omics1\n",
    "    adj_spatial_omics1 = adata.uns['adj_spatial']\n",
    "    adj_spatial_omics1 = transform_adjacent_matrix(adj_spatial_omics1)\n",
    "    adj_spatial_omics1 = adj_spatial_omics1.toarray()\n",
    "    adj_spatial_omics1 = adj_spatial_omics1 + adj_spatial_omics1.T\n",
    "    adj_spatial_omics1 = np.where(adj_spatial_omics1>1, 1, adj_spatial_omics1)\n",
    "    adj = preprocess_graph(adj_spatial_omics1)\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = adata_omics1.obs['cluster'].astype(str) #先全部转成string  \n",
    "label[pd.isna(label)] = \"nan\"  # 将缺失值替换为 \"NA\"，前提是你用了pandas\n",
    "# label=adata_omics1.obs['ground_truth'].values\n",
    "classes, label = np.unique(label, return_inverse=True)\n",
    "classes = classes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda:0'\n",
    "args.n_input=adata_omics1.n_vars\n",
    "# args.n_input1=adata_omics2.n_vars-1\n",
    "args.n_clusters=len(classes)\n",
    "args.n_clusters\n",
    "args.Type='Stereo-CITE-seq'\n",
    "args.n_clusters=len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed=2020\n",
    "args.random_seed=random_seed\n",
    "fix_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.tool='kmeans'\n",
    "args.sigma=0.5\n",
    "args.lr=0.01\n",
    "args.loss_n=0.1\n",
    "args.loss_a=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool: kmeans\n",
      "  9 loss: 1.0617234706878662\n",
      "Epoch_  9 , nmi 0.2005 , ari 0.1580 , ami 0.1978 , homogeneity 0.2220 , completeness 0.1828 , v_measure 0.2005\n",
      " 19 loss: 0.9829371571540833\n",
      "Epoch_ 19 , nmi 0.2537 , ari 0.2170 , ami 0.2511 , homogeneity 0.2757 , completeness 0.2348 , v_measure 0.2537\n",
      " 29 loss: 1.5594666004180908\n",
      "Epoch_ 29 , nmi 0.4530 , ari 0.4589 , ami 0.4511 , homogeneity 0.5031 , completeness 0.4120 , v_measure 0.4530\n",
      " 39 loss: 1.510082483291626\n",
      "Epoch_ 39 , nmi 0.5238 , ari 0.5108 , ami 0.5222 , homogeneity 0.5767 , completeness 0.4798 , v_measure 0.5238\n",
      " 49 loss: 1.465939998626709\n",
      "Epoch_ 49 , nmi 0.5618 , ari 0.5692 , ami 0.5603 , homogeneity 0.6168 , completeness 0.5158 , v_measure 0.5618\n",
      " 59 loss: 1.4348514080047607\n",
      "Epoch_ 59 , nmi 0.5728 , ari 0.5718 , ami 0.5713 , homogeneity 0.6294 , completeness 0.5255 , v_measure 0.5728\n",
      " 69 loss: 1.3891575336456299\n",
      "Epoch_ 69 , nmi 0.5422 , ari 0.5213 , ami 0.5406 , homogeneity 0.5999 , completeness 0.4946 , v_measure 0.5422\n",
      " 79 loss: 1.3632278442382812\n",
      "Epoch_ 79 , nmi 0.5628 , ari 0.5672 , ami 0.5613 , homogeneity 0.6150 , completeness 0.5187 , v_measure 0.5628\n",
      " 89 loss: 1.3228492736816406\n",
      "Epoch_ 89 , nmi 0.5393 , ari 0.5281 , ami 0.5378 , homogeneity 0.5951 , completeness 0.4931 , v_measure 0.5393\n",
      " 99 loss: 1.2891623973846436\n",
      "Epoch_ 99 , nmi 0.5485 , ari 0.5444 , ami 0.5470 , homogeneity 0.6060 , completeness 0.5010 , v_measure 0.5485\n",
      "109 loss: 1.2660455703735352\n",
      "Epoch_109 , nmi 0.5516 , ari 0.5456 , ami 0.5500 , homogeneity 0.6053 , completeness 0.5066 , v_measure 0.5516\n",
      "119 loss: 1.2378008365631104\n",
      "Epoch_119 , nmi 0.5748 , ari 0.5801 , ami 0.5734 , homogeneity 0.6263 , completeness 0.5311 , v_measure 0.5748\n",
      "129 loss: 1.2192480564117432\n",
      "Epoch_129 , nmi 0.5553 , ari 0.5559 , ami 0.5538 , homogeneity 0.6074 , completeness 0.5115 , v_measure 0.5553\n",
      "139 loss: 1.2015936374664307\n",
      "Epoch_139 , nmi 0.5682 , ari 0.5744 , ami 0.5667 , homogeneity 0.6191 , completeness 0.5251 , v_measure 0.5682\n",
      "149 loss: 1.1828159093856812\n",
      "Epoch_149 , nmi 0.5568 , ari 0.5683 , ami 0.5553 , homogeneity 0.6079 , completeness 0.5137 , v_measure 0.5568\n",
      "159 loss: 1.1717197895050049\n",
      "Epoch_159 , nmi 0.5661 , ari 0.5647 , ami 0.5646 , homogeneity 0.6195 , completeness 0.5212 , v_measure 0.5661\n",
      "169 loss: 1.1542940139770508\n",
      "Epoch_169 , nmi 0.5541 , ari 0.5837 , ami 0.5526 , homogeneity 0.6002 , completeness 0.5146 , v_measure 0.5541\n",
      "179 loss: 1.1441545486450195\n",
      "Epoch_179 , nmi 0.5569 , ari 0.5671 , ami 0.5554 , homogeneity 0.6093 , completeness 0.5128 , v_measure 0.5569\n",
      "189 loss: 1.1313310861587524\n",
      "Epoch_189 , nmi 0.5511 , ari 0.5539 , ami 0.5496 , homogeneity 0.6040 , completeness 0.5068 , v_measure 0.5511\n",
      "199 loss: 1.1213750839233398\n",
      "Epoch_199 , nmi 0.5440 , ari 0.5714 , ami 0.5424 , homogeneity 0.5930 , completeness 0.5025 , v_measure 0.5440\n",
      "xunlian\n",
      "name: acm\n",
      "NMI : 0.5748\n",
      "ARI : 0.5801\n",
      "AMI  : 0.5734\n",
      "Searching resolution using binary search...\n",
      "resolution=3.5005, cluster number=38\n",
      "resolution=1.7508, cluster number=16\n",
      "resolution=0.8759, cluster number=8\n",
      "resolution=0.4384, cluster number=3\n",
      "resolution=0.6572, cluster number=6\n",
      "resolution=0.5478, cluster number=6\n",
      "resolution=0.4931, cluster number=4\n",
      "Epoch_  0 , nmi 0.5664 , ari 0.4905 , ami 0.5649 , homogeneity 0.6246 , completeness 0.5182 , v_measure 0.5664\n",
      "聚类方法为leiden\n",
      "test\n",
      "name: acm\n",
      "NMI : 0.5664\n",
      "ARI : 0.4905\n",
      "AMI  : 0.5649\n"
     ]
    }
   ],
   "source": [
    "args.n_clusters1=len(set(label))\n",
    "args.n_clusters2=len(set(label))\n",
    "adj_train=create_adj(adata_omics1)\n",
    "adj_train = adj_train.to(device)\n",
    "features_omics1_train = torch.FloatTensor(adata_omics1.X.copy()).to(device)\n",
    "model = creat_model('spamgcn', args).to(device)\n",
    "model=Train(200, model,adata_omics1, features_omics1_train, adj_train, label, device, args)\n",
    "nmi, ari, ami, homogeneity, completeness, v_measure=Test(model,adata_omics1,features_omics1_train,adj_train,label,device,args,'leiden')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GraphST",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
